import asyncio
import json
import os
import re
import tempfile
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from email.utils import parsedate_to_datetime
from typing import List, Dict, Set, Optional

import aiohttp

from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.event import MessageChain
from astrbot.api import AstrBotConfig
from astrbot.api.star import Context, Star, register
from astrbot.api.star import StarTools
from astrbot.api import logger

# RSS è®¢é˜…æºé…ç½®
RSS_URL = "https://imjuya.github.io/juya-ai-daily/rss.xml"

# AI æ€»ç»“ prompt
SUMMARY_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI èµ„è®¯ç¼–è¾‘ã€‚è¯·å°†ä»¥ä¸‹ AI æ—©æŠ¥å†…å®¹è¿›è¡Œç²¾ç‚¼æ€»ç»“ï¼Œè¦æ±‚ï¼š
1. æå–æœ€é‡è¦çš„ 5-8 æ¡æ–°é—»è¦ç‚¹
2. æ¯æ¡ç”¨ä¸€å¥è¯æ¦‚æ‹¬ï¼Œçªå‡ºå…³é”®ä¿¡æ¯ï¼ˆå…¬å¸ã€äº§å“ã€æŠ€æœ¯ã€æ•°æ®ï¼‰
3. ä½¿ç”¨ç®€æ´çš„ä¸­æ–‡è¡¨è¿°
4. åœ¨å¼€å¤´åŠ ä¸Šæ—¥æœŸ
5. ä¿æŒæ–°é—»çš„æ—¶æ•ˆæ€§å’Œå‡†ç¡®æ€§

åŸæ–‡å†…å®¹ï¼š
{content}

è¯·è¾“å‡ºæ€»ç»“ï¼š"""


@register(
    "astrbot_plugin_daily_ai_news",
    "xx",
    "è®¢é˜…æ©˜é¸¦AIæ—¥æŠ¥å¹¶è¿›è¡ŒAIæ€»ç»“",
    "3.0.0",
    "https://github.com/xxofficial/astrbot_plugin_daily_ai_news",
)
class DailyAINewsPlugin(Star):
    def __init__(self, context: Context, config: AstrBotConfig):
        super().__init__(context)
        self.config = config
        self._task: Optional[asyncio.Task] = None

        # ä½¿ç”¨æ¡†æ¶è§„èŒƒçš„æ•°æ®ç›®å½•
        self._data_dir = StarTools.get_data_dir("astrbot_plugin_daily_ai_news")
        self._subscriptions_file = self._data_dir / "subscriptions.json"
        self._sent_file = self._data_dir / "sent_news.json"
        self._cache_file = self._data_dir / "summary_cache.json"

        # é€šè¿‡æŒ‡ä»¤è®¢é˜…çš„ unified_msg_origin é›†åˆ
        self._cmd_subscriptions: Set[str] = set()
        # å·²æ¨é€çš„æ—¥æœŸå’Œé“¾æ¥ï¼ˆåˆ†ç¦»å­˜å‚¨ï¼‰
        self._sent_dates: Set[str] = set()
        self._sent_links: Set[str] = set()

        # æ–‡ä»¶è¯»å†™äº’æ–¥é”
        self._file_lock = asyncio.Lock()

    async def initialize(self):
        """æ’ä»¶åˆå§‹åŒ–ï¼šåŠ è½½æŒä¹…åŒ–æ•°æ®ï¼Œå¯åŠ¨å®šæ—¶æ¨é€ä»»åŠ¡ã€‚"""
        os.makedirs(self._data_dir, exist_ok=True)
        await self._load_subscriptions()
        await self._load_sent_news()

        self._task = asyncio.create_task(self._schedule_loop())
        logger.info("æ¯æ—¥AIèµ„è®¯æ¨é€æ’ä»¶å·²åˆå§‹åŒ–ï¼ˆRSS è®¢é˜… + AI æ€»ç»“æ¨¡å¼ï¼‰")

    # ==================== æŒ‡ä»¤å¤„ç† ====================

    @filter.command("ainews")
    async def cmd_ainews(self, event: AstrMessageEvent):
        """æ‰‹åŠ¨è·å–æœ€æ–° AI æ—©æŠ¥"""
        yield event.plain_result("ğŸ”„ æ­£åœ¨ä» RSS è·å–æœ€æ–° AI æ—©æŠ¥ï¼Œè¯·ç¨å€™...")
        article = await self._fetch_rss_latest()
        if not article:
            yield event.plain_result("ğŸ˜ æš‚æ—¶æœªèƒ½è·å–åˆ° AI æ—©æŠ¥ï¼Œè¯·ç¨åå†è¯•ã€‚")
            return

        # ä½¿ç”¨æ–‡ç« å®é™…æ—¥æœŸä½œä¸ºç¼“å­˜ keyï¼Œè€Œéå½“å¤©æ—¥æœŸ
        article_date = self._parse_article_date(article)

        # æ£€æŸ¥ç¼“å­˜
        cache = await self._read_summary_cache()
        cached = cache.get(article_date)
        if cached:
            logger.info(f"ä½¿ç”¨ç¼“å­˜çš„ AI æ€»ç»“ ({article_date})")
            text = self._format_summary(
                cached["title"], cached["url"], cached["summary"], article_date
            )
            yield event.plain_result(text)
            return

        text = await self._get_or_create_summary(article, article_date)
        if not text:
            yield event.plain_result("ğŸ˜ AI æ€»ç»“å¤±è´¥ï¼Œè¯·ç¨åå†è¯•ã€‚")
            return
        yield event.plain_result(text)

    @filter.command("ainews_sub")
    async def cmd_subscribe(self, event: AstrMessageEvent):
        """è®¢é˜…æ¯æ—¥ AI èµ„è®¯æ¨é€ï¼ˆåœ¨ç¾¤èŠä¸­ä½¿ç”¨ï¼‰"""
        umo = event.unified_msg_origin
        if umo in self._cmd_subscriptions:
            yield event.plain_result("ğŸ“¢ å½“å‰ä¼šè¯å·²è®¢é˜…æ¯æ—¥AIèµ„è®¯æ¨é€ã€‚")
            return
        self._cmd_subscriptions.add(umo)
        await self._save_subscriptions()
        yield event.plain_result(
            "âœ… è®¢é˜…æˆåŠŸï¼æ¯æ—¥å°†è‡ªåŠ¨æ¨é€ AI æ—©æŠ¥æ€»ç»“åˆ°æœ¬ç¾¤ã€‚\n"
            "å–æ¶ˆè®¢é˜…è¯·å‘é€ /ainews_unsub"
        )

    @filter.command("ainews_unsub")
    async def cmd_unsubscribe(self, event: AstrMessageEvent):
        """å–æ¶ˆæ¯æ—¥ AI èµ„è®¯æ¨é€è®¢é˜…"""
        umo = event.unified_msg_origin
        if umo not in self._cmd_subscriptions:
            yield event.plain_result("â„¹ï¸ å½“å‰ä¼šè¯æœªé€šè¿‡æŒ‡ä»¤è®¢é˜…è¿‡ AI èµ„è®¯æ¨é€ã€‚")
            return
        self._cmd_subscriptions.discard(umo)
        await self._save_subscriptions()
        yield event.plain_result("âœ… å·²å–æ¶ˆæ¯æ—¥AIèµ„è®¯æ¨é€è®¢é˜…ã€‚")

    @filter.command("ainews_status")
    async def cmd_status(self, event: AstrMessageEvent):
        """æŸ¥çœ‹æ¨é€çŠ¶æ€"""
        hour = self.config.get("push_hour", 8)
        minute = self.config.get("push_minute", 0)
        poll_interval = self.config.get("rss_poll_interval", 600)
        cmd_sub_count = len(self._cmd_subscriptions)
        cfg_groups = self._get_config_groups()
        cfg_group_count = len(cfg_groups)
        cfg_users = self._get_config_users()
        cfg_user_count = len(cfg_users)

        status_text = (
            "ğŸ“Š **æ¯æ—¥AIèµ„è®¯æ¨é€çŠ¶æ€**\n"
            f"ğŸ“¡ æ•°æ®æºï¼šRSS è®¢é˜…ï¼ˆæ©˜é¸¦ AI æ—¥æŠ¥ï¼‰\n"
            f"â° é¦–æ¬¡æ£€æŸ¥æ—¶é—´ï¼šæ¯å¤© {hour:02d}:{minute:02d}\n"
            f"ğŸ”„ è½®è¯¢é—´éš”ï¼š{poll_interval} ç§’\n"
            f"ğŸ¤– AI æ€»ç»“ï¼šå·²å¯ç”¨\n"
            f"ğŸ“‹ æŒ‡ä»¤è®¢é˜…æ•°ï¼š{cmd_sub_count}\n"
            f"ğŸ“‹ é…ç½®ç¾¤å·æ•°ï¼š{cfg_group_count}\n"
            f"ğŸ“‹ é…ç½®ç§èŠæ•°ï¼š{cfg_user_count}\n"
            f"ğŸ“š å·²æ¨é€æ—¥æœŸç¼“å­˜ï¼š{len(self._sent_dates)} å¤©\n"
            f"ğŸ“š å·²æ¨é€æ–‡ç« ç¼“å­˜ï¼š{len(self._sent_links)} ç¯‡"
        )
        yield event.plain_result(status_text)

    # ==================== å®šæ—¶ + è½®è¯¢æ¨é€ ====================

    async def _schedule_loop(self):
        """åå°å®šæ—¶å¾ªç¯ï¼šæ¯å¤©åœ¨è®¾å®šæ—¶é—´é¦–æ¬¡æ£€æŸ¥ RSSï¼Œè‹¥æœªæ›´æ–°åˆ™è½®è¯¢ç›´åˆ°è·å–åˆ°å½“æ—¥æ–‡ç« ã€‚"""
        # å¯åŠ¨æ—¶å…ˆæ‰§è¡Œä¸€æ¬¡è¡¥å¿æ£€æŸ¥
        await self._startup_compensation_check()

        while True:
            try:
                target_hour = self.config.get("push_hour", 8)
                target_minute = self.config.get("push_minute", 0)
                poll_interval = self.config.get("rss_poll_interval", 600)

                now = datetime.now()
                target = now.replace(
                    hour=target_hour, minute=target_minute, second=0, microsecond=0
                )
                if target <= now:
                    target += timedelta(days=1)

                wait_seconds = (target - now).total_seconds()
                logger.info(
                    f"ä¸‹æ¬¡ RSS æ£€æŸ¥æ—¶é—´ï¼š{target.strftime('%Y-%m-%d %H:%M')}ï¼Œ"
                    f"ç­‰å¾… {wait_seconds:.0f} ç§’"
                )

                await asyncio.sleep(wait_seconds)

                # åˆ°è¾¾è®¾å®šæ—¶é—´ï¼Œå¼€å§‹æ£€æŸ¥ RSS å¹¶å°è¯•æ¨é€
                today = datetime.now().strftime("%Y-%m-%d")

                # æ£€æŸ¥ä»Šå¤©æ˜¯å¦å·²ç»æ¨é€è¿‡
                if today in self._sent_dates:
                    logger.info(f"ä»Šæ—¥ ({today}) å·²æ¨é€è¿‡ï¼Œç­‰å¾…æ˜å¤©")
                    continue

                # é¦–æ¬¡å°è¯•è·å– RSS
                pushed = await self._try_fetch_and_push(today)
                if pushed:
                    continue

                # RSS å°šæœªæ›´æ–°ï¼Œè¿›å…¥è½®è¯¢æ¨¡å¼
                logger.info(
                    f"RSS å°šæœªæ›´æ–°å½“æ—¥ ({today}) å†…å®¹ï¼Œ"
                    f"è¿›å…¥è½®è¯¢æ¨¡å¼ï¼ˆé—´éš” {poll_interval} ç§’ï¼‰"
                )
                while True:
                    await asyncio.sleep(poll_interval)

                    # å¦‚æœå·²ç»è¿‡äº†å½“å¤©ï¼Œåœæ­¢è½®è¯¢
                    current_date = datetime.now().strftime("%Y-%m-%d")
                    if current_date != today:
                        logger.info("å·²è¿‡å½“å¤©ï¼Œåœæ­¢è½®è¯¢ï¼Œç­‰å¾…æ˜å¤©å®šæ—¶è§¦å‘")
                        break

                    pushed = await self._try_fetch_and_push(today)
                    if pushed:
                        break

            except asyncio.CancelledError:
                logger.info("å®šæ—¶æ¨é€ä»»åŠ¡å·²å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(f"å®šæ—¶æ¨é€ä»»åŠ¡å‡ºé”™: {e}")
                await asyncio.sleep(60)

    async def _startup_compensation_check(self):
        """å¯åŠ¨æ—¶è¡¥å¿æ£€æŸ¥ï¼šè‹¥å½“å‰å·²è¿‡æ¨é€æ—¶é—´ä¸”å½“å¤©æœªæ¨é€è¿‡ï¼Œç«‹å³å°è¯•æ¨é€ã€‚"""
        try:
            target_hour = self.config.get("push_hour", 8)
            target_minute = self.config.get("push_minute", 0)

            now = datetime.now()
            today = now.strftime("%Y-%m-%d")

            # åªåœ¨è¿‡äº†ä»Šå¤©çš„æ¨é€æ—¶é—´åæ‰è¡¥å¿
            target_time = now.replace(
                hour=target_hour, minute=target_minute, second=0, microsecond=0
            )
            if now < target_time:
                logger.info("å½“å‰æœªåˆ°æ¨é€æ—¶é—´ï¼Œè·³è¿‡è¡¥å¿æ£€æŸ¥")
                return

            if today in self._sent_dates:
                logger.info(f"ä»Šæ—¥ ({today}) å·²æ¨é€è¿‡ï¼Œè·³è¿‡è¡¥å¿æ£€æŸ¥")
                return

            logger.info(f"å¯åŠ¨è¡¥å¿æ£€æŸ¥ï¼šä»Šæ—¥ ({today}) å°šæœªæ¨é€ï¼Œå°è¯•æ‹‰å–å¹¶æ¨é€")
            await self._try_fetch_and_push(today)

        except Exception as e:
            logger.error(f"å¯åŠ¨è¡¥å¿æ£€æŸ¥å¤±è´¥: {e}")

    async def _try_fetch_and_push(self, today: str) -> bool:
        """å°è¯•ä» RSS è·å–å½“æ—¥æ–‡ç« å¹¶æ¨é€ã€‚è¿”å› True è¡¨ç¤ºæˆåŠŸæ¨é€ã€‚"""
        try:
            article = await self._fetch_rss_latest()
            if not article:
                logger.info("RSS è·å–å¤±è´¥æˆ–æ— æ–‡ç« ")
                return False

            # è§£ææ–‡ç« æ—¥æœŸ
            article_date = self._parse_article_date(article)

            # æ£€æŸ¥æ˜¯å¦æ˜¯å½“æ—¥æ–‡ç« 
            if article_date != today:
                logger.info(
                    f"RSS æœ€æ–°æ–‡ç« æ—¥æœŸ ({article_date}) ä¸æ˜¯ä»Šæ—¥ ({today})ï¼Œç»§ç»­ç­‰å¾…"
                )
                return False

            # æ£€æŸ¥æ˜¯å¦å·²æ¨é€è¿‡è¯¥æ–‡ç« ï¼ˆåŸºäºé“¾æ¥å»é‡ï¼‰
            if article["link"] in self._sent_links:
                logger.info(f"è¯¥æ–‡ç« å·²æ¨é€è¿‡ï¼š{article['link']}")
                return False

            # è·å– AI æ€»ç»“å¹¶æ¨é€
            await self._do_push(article, article_date)
            return True

        except Exception as e:
            logger.error(f"å°è¯•è·å–å¹¶æ¨é€å¤±è´¥: {e}")
            return False

    async def _do_push(self, article: Dict, article_date: str):
        """æ‰§è¡Œä¸€æ¬¡æ–°é—»æ¨é€åˆ°æ‰€æœ‰è®¢é˜…ç›®æ ‡ã€‚"""
        logger.info(f"å¼€å§‹æ‰§è¡Œæ¯æ—¥AIèµ„è®¯æ¨é€: {article['title']}")

        text = await self._get_or_create_summary(article, article_date)
        if not text:
            logger.warning("æœªèƒ½ç”Ÿæˆ AI æ€»ç»“ï¼Œè·³è¿‡æœ¬æ¬¡æ¨é€")
            return

        # æ¨é€
        targets = self._get_all_targets()
        if not targets:
            logger.info("æ²¡æœ‰ä»»ä½•æ¨é€ç›®æ ‡ï¼Œè·³è¿‡æ¨é€")
            return

        success_count = 0
        for umo in targets:
            try:
                chain = MessageChain().message(text)
                await self.context.send_message(umo, chain)
                logger.info(f"å·²æ¨é€è‡³: {umo}")
                success_count += 1
            except Exception as e:
                logger.error(f"æ¨é€åˆ° {umo} å¤±è´¥: {e}")

        # ä»…åœ¨è‡³å°‘ä¸€ä¸ªç›®æ ‡å‘é€æˆåŠŸåæ‰æ ‡è®°å·²æ¨é€
        if success_count > 0:
            self._sent_dates.add(article_date)
            self._sent_links.add(article["link"])
            # è£å‰ªï¼šæ—¥æœŸä¿ç•™æœ€è¿‘ 30 å¤©ï¼Œé“¾æ¥ä¿ç•™æœ€è¿‘ 100 æ¡
            if len(self._sent_dates) > 30:
                sorted_dates = sorted(self._sent_dates)
                self._sent_dates = set(sorted_dates[-30:])
            if len(self._sent_links) > 100:
                self._sent_links = set(list(self._sent_links)[-100:])
            await self._save_sent_news()
            logger.info(
                f"æ¯æ—¥AIèµ„è®¯æ¨é€å®Œæˆï¼ŒæˆåŠŸæ¨é€åˆ° {success_count}/{len(targets)} ä¸ªç›®æ ‡"
            )
        else:
            logger.warning("æ‰€æœ‰æ¨é€ç›®æ ‡å‡å¤±è´¥ï¼Œä¸æ ‡è®°å·²æ¨é€ï¼Œåç»­å°†é‡è¯•")

    async def _get_or_create_summary(
        self, article: Dict, article_date: str
    ) -> Optional[str]:
        """è·å–æŒ‡å®šæ—¥æœŸçš„ AI æ€»ç»“ï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜ã€‚"""
        # æ£€æŸ¥ç¼“å­˜
        cache = await self._read_summary_cache()
        cached = cache.get(article_date)
        if cached:
            logger.info(f"ä½¿ç”¨ç¼“å­˜çš„ AI æ€»ç»“ ({article_date})")
            return self._format_summary(
                cached["title"], cached["url"], cached["summary"], article_date
            )

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œè¿›è¡Œ AI æ€»ç»“
        summary = await self._summarize_with_ai(article["content"])
        if summary:
            # å†™å…¥ç¼“å­˜
            cache = await self._read_summary_cache()
            cache[article_date] = {
                "title": article["title"],
                "url": article["link"],
                "summary": summary,
            }
            await self._save_summary_cache(cache)
            return self._format_summary(
                article["title"], article["link"], summary, article_date
            )
        else:
            return self._format_fallback(article, article_date)

    # ==================== RSS è·å– ====================

    async def _fetch_rss_latest(self) -> Optional[Dict]:
        """ä» RSS è®¢é˜…æºè·å–æœ€æ–°ä¸€ç¯‡æ–‡ç« ã€‚"""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (compatible; AstrBot/3.0; +https://github.com/AstrBot)",
                "Accept": "application/rss+xml, application/xml, text/xml, */*",
            }

            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30)
            ) as session:
                async with session.get(RSS_URL, headers=headers) as resp:
                    if resp.status != 200:
                        logger.warning(f"RSS è¯·æ±‚è¿”å›çŠ¶æ€ç  {resp.status}")
                        return None

                    xml_text = await resp.text()

            # è§£æ RSS XML
            root = ET.fromstring(xml_text)
            channel = root.find("channel")
            if channel is None:
                logger.warning("RSS XML ä¸­æœªæ‰¾åˆ° channel å…ƒç´ ")
                return None

            # è·å–ç¬¬ä¸€ä¸ª itemï¼ˆæœ€æ–°æ–‡ç« ï¼‰
            item = channel.find("item")
            if item is None:
                logger.warning("RSS ä¸­æ²¡æœ‰ä»»ä½•æ–‡ç« ")
                return None

            title = item.findtext("title", "").strip()
            link = item.findtext("link", "").strip()
            description = item.findtext("description", "").strip()
            pub_date = item.findtext("pubDate", "").strip()

            if not title:
                logger.warning("RSS æ–‡ç« æ ‡é¢˜ä¸ºç©º")
                return None

            # æ¸…ç† HTMLï¼ˆdescription å¯èƒ½åŒ…å« HTML æ ‡ç­¾ï¼‰
            content = self._clean_html(description)

            logger.info(f"RSS è·å–åˆ°æœ€æ–°æ–‡ç« ï¼š{title}")
            return {
                "title": title,
                "link": link,
                "content": content,
                "pub_date": pub_date,
            }

        except ET.ParseError as e:
            logger.error(f"RSS XML è§£æå¤±è´¥: {e}")
        except Exception as e:
            logger.error(f"RSS è·å–å¤±è´¥: {e}")

        return None

    def _parse_article_date(self, article: Dict) -> str:
        """ä»æ–‡ç« ä¸­è§£ææ—¥æœŸï¼Œä¼˜å…ˆä½¿ç”¨ pubDateï¼Œå›é€€ä»æ ‡é¢˜æå–ï¼Œæœ€åä½¿ç”¨å½“å¤©æ—¥æœŸã€‚"""
        # ä¼˜å…ˆï¼šè§£æ pubDateï¼ˆRFC 2822 æ ¼å¼ï¼‰
        pub_date = article.get("pub_date", "")
        if pub_date:
            try:
                dt = parsedate_to_datetime(pub_date)
                return dt.strftime("%Y-%m-%d")
            except Exception:
                pass

        # å›é€€ï¼šä»æ ‡é¢˜æå– YYYY-MM-DD
        title = article.get("title", "")
        match = re.search(r"(\d{4}-\d{2}-\d{2})", title)
        if match:
            return match.group(1)

        # æœ€åå›é€€ï¼šå½“å¤©æ—¥æœŸ
        return datetime.now().strftime("%Y-%m-%d")

    # ==================== AI æ€»ç»“ ====================

    async def _summarize_with_ai(self, content: str) -> Optional[str]:
        """ä½¿ç”¨ AstrBot å†…ç½® LLM å¯¹å†…å®¹è¿›è¡Œæ€»ç»“ã€‚"""
        if not content or len(content.strip()) < 50:
            logger.warning("æ–‡ç« å†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡ AI æ€»ç»“")
            return None

        try:
            # å†…å®¹è¿‡é•¿æ—¶æˆªæ–­ï¼Œé¿å…è¶…è¿‡æ¨¡å‹ä¸Šä¸‹æ–‡é™åˆ¶
            max_len = 8000
            if len(content) > max_len:
                content = content[:max_len] + "\n...(å†…å®¹è¿‡é•¿å·²æˆªæ–­)"

            prompt = SUMMARY_PROMPT.format(content=content)

            # ä½¿ç”¨ AstrBot æä¾›çš„ LLM æ¥å£
            provider = self.context.get_using_provider()
            if provider is None:
                logger.warning("æœªé…ç½® LLM providerï¼Œæ— æ³•è¿›è¡Œ AI æ€»ç»“")
                return None

            resp = await provider.text_chat(
                prompt=prompt,
                session_id="ainews_summary",
            )

            if resp and resp.completion_text:
                return resp.completion_text.strip()
            else:
                logger.warning("LLM è¿”å›ç»“æœä¸ºç©º")
                return None

        except Exception as e:
            logger.error(f"AI æ€»ç»“å¤±è´¥: {e}")
            return None

    # ==================== æ ¼å¼åŒ–è¾“å‡º ====================

    def _format_summary(
        self, title: str, url: str, summary: str, article_date: str
    ) -> str:
        """æ ¼å¼åŒ– AI æ€»ç»“åçš„æ¨é€æ–‡æœ¬ã€‚"""
        return (
            f"ğŸ“° AI æ—©æŠ¥é€Ÿé€’ | {article_date}\n"
            f"{'=' * 28}\n\n"
            f"ğŸ“Œ åŸæ–‡ï¼š{title}\n\n"
            f"ğŸ¤– AI æ€»ç»“ï¼š\n\n"
            f"{summary}\n\n"
            f"{'=' * 28}\n"
            f"ğŸ”— åŸæ–‡é“¾æ¥ï¼š{url}\n"
            f"ğŸ’¡ å‘é€ /ainews éšæ—¶è·å–æœ€æ–°èµ„è®¯"
        )

    def _format_fallback(self, article: Dict, article_date: str) -> str:
        """å½“ AI æ€»ç»“å¤±è´¥æ—¶ï¼Œä½¿ç”¨åŸæ–‡æ‘˜è¦ã€‚"""
        content = article.get("content", "")
        if len(content) > 500:
            content = content[:500] + "..."

        return (
            f"ğŸ“° AI æ—©æŠ¥ | {article_date}\n"
            f"{'=' * 28}\n\n"
            f"ğŸ“Œ {article['title']}\n\n"
            f"{content}\n\n"
            f"{'=' * 28}\n"
            f"ğŸ”— åŸæ–‡é“¾æ¥ï¼š{article.get('link', '')}\n"
            f"ğŸ’¡ å‘é€ /ainews éšæ—¶è·å–æœ€æ–°èµ„è®¯"
        )

    # ==================== å·¥å…·æ–¹æ³• ====================

    def _clean_html(self, text: str) -> str:
        """å»é™¤ HTML æ ‡ç­¾ï¼Œè½¬ä¸ºçº¯æ–‡æœ¬ã€‚"""
        if not text:
            return ""
        clean = re.sub(r"<[^>]+>", "", text)
        clean = clean.replace("&nbsp;", " ").replace("&amp;", "&")
        clean = clean.replace("&lt;", "<").replace("&gt;", ">")
        clean = clean.replace("&quot;", '"')
        clean = re.sub(r"\n{3,}", "\n\n", clean)
        return clean.strip()

    def _get_config_groups(self) -> List[str]:
        """ä»é…ç½®ä¸­è·å–æ‰‹åŠ¨å¡«å†™çš„ QQ ç¾¤å·åˆ—è¡¨ã€‚"""
        groups_text = self.config.get("subscribed_groups", "")
        if not groups_text or not groups_text.strip():
            return []
        return [g.strip() for g in groups_text.strip().split("\n") if g.strip()]

    def _get_config_users(self) -> List[str]:
        """ä»é…ç½®ä¸­è·å–æ‰‹åŠ¨å¡«å†™çš„ç§èŠ QQ å·åˆ—è¡¨ã€‚"""
        users_text = self.config.get("subscribed_users", "")
        if not users_text or not users_text.strip():
            return []
        return [u.strip() for u in users_text.strip().split("\n") if u.strip()]

    def _get_all_targets(self) -> Set[str]:
        """è·å–æ‰€æœ‰æ¨é€ç›®æ ‡ã€‚"""
        targets = set(self._cmd_subscriptions)

        cfg_groups = self._get_config_groups()
        for group_id in cfg_groups:
            umo = f"aiocqhttp:GroupMessage:{group_id}"
            targets.add(umo)

        cfg_users = self._get_config_users()
        for user_id in cfg_users:
            umo = f"aiocqhttp:FriendMessage:{user_id}"
            targets.add(umo)

        return targets

    # ==================== æŒä¹…åŒ–ï¼ˆå¸¦é” + åŸå­å†™ï¼‰====================

    def _atomic_write(self, filepath, data: dict):
        """åŸå­å†™å…¥ JSON æ–‡ä»¶ï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼Œå† rename æ›¿æ¢ã€‚"""
        dir_path = os.path.dirname(str(filepath))
        try:
            fd, tmp_path = tempfile.mkstemp(dir=dir_path, suffix=".tmp")
            try:
                with os.fdopen(fd, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                os.replace(tmp_path, str(filepath))
            except Exception:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(tmp_path)
                except OSError:
                    pass
                raise
        except Exception as e:
            logger.error(f"åŸå­å†™å…¥ {filepath} å¤±è´¥: {e}")
            raise

    async def _load_subscriptions(self):
        """ä»æ–‡ä»¶åŠ è½½æŒ‡ä»¤è®¢é˜…åˆ—è¡¨ã€‚"""
        async with self._file_lock:
            try:
                filepath = str(self._subscriptions_file)
                if os.path.exists(filepath):
                    with open(filepath, "r", encoding="utf-8") as f:
                        data = json.load(f)
                    self._cmd_subscriptions = set(data.get("subscriptions", []))
                    logger.info(f"å·²åŠ è½½ {len(self._cmd_subscriptions)} ä¸ªæŒ‡ä»¤è®¢é˜…")
            except Exception as e:
                logger.error(f"åŠ è½½è®¢é˜…åˆ—è¡¨å¤±è´¥: {e}")
                self._cmd_subscriptions = set()

    async def _save_subscriptions(self):
        """å°†æŒ‡ä»¤è®¢é˜…åˆ—è¡¨ä¿å­˜åˆ°æ–‡ä»¶ã€‚"""
        async with self._file_lock:
            try:
                self._atomic_write(
                    self._subscriptions_file,
                    {"subscriptions": list(self._cmd_subscriptions)},
                )
            except Exception as e:
                logger.error(f"ä¿å­˜è®¢é˜…åˆ—è¡¨å¤±è´¥: {e}")

    async def _load_sent_news(self):
        """åŠ è½½å·²æ¨é€è®°å½•ã€‚"""
        async with self._file_lock:
            try:
                filepath = str(self._sent_file)
                if os.path.exists(filepath):
                    with open(filepath, "r", encoding="utf-8") as f:
                        data = json.load(f)
                    # å…¼å®¹æ—§æ ¼å¼ï¼šå¦‚æœæ˜¯æ—§çš„ sent_ids æ ¼å¼ï¼Œè‡ªåŠ¨è¿ç§»
                    if "sent_ids" in data:
                        old_ids = set(data.get("sent_ids", []))
                        for item in old_ids:
                            if re.match(r"\d{4}-\d{2}-\d{2}$", item):
                                self._sent_dates.add(item)
                            else:
                                self._sent_links.add(item)
                        logger.info("å·²ä»æ—§æ ¼å¼è¿ç§»å·²æ¨é€è®°å½•")
                    else:
                        self._sent_dates = set(data.get("sent_dates", []))
                        self._sent_links = set(data.get("sent_links", []))
                    logger.info(
                        f"å·²åŠ è½½ {len(self._sent_dates)} ä¸ªå·²æ¨é€æ—¥æœŸï¼Œ"
                        f"{len(self._sent_links)} ä¸ªå·²æ¨é€é“¾æ¥"
                    )
            except Exception as e:
                logger.error(f"åŠ è½½å·²æ¨é€è®°å½•å¤±è´¥: {e}")
                self._sent_dates = set()
                self._sent_links = set()

    async def _save_sent_news(self):
        """ä¿å­˜å·²æ¨é€è®°å½•ã€‚"""
        async with self._file_lock:
            try:
                self._atomic_write(
                    self._sent_file,
                    {
                        "sent_dates": sorted(self._sent_dates),
                        "sent_links": list(self._sent_links),
                    },
                )
            except Exception as e:
                logger.error(f"ä¿å­˜å·²æ¨é€è®°å½•å¤±è´¥: {e}")

    async def _read_summary_cache(self) -> Dict[str, Dict]:
        """ä»æ–‡ä»¶è¯»å– AI æ€»ç»“ç¼“å­˜ã€‚"""
        async with self._file_lock:
            try:
                filepath = str(self._cache_file)
                if os.path.exists(filepath):
                    with open(filepath, "r", encoding="utf-8") as f:
                        return json.load(f)
            except Exception as e:
                logger.error(f"è¯»å–æ€»ç»“ç¼“å­˜å¤±è´¥: {e}")
            return {}

    async def _save_summary_cache(self, cache: Dict[str, Dict]):
        """ä¿å­˜ AI æ€»ç»“ç¼“å­˜åˆ°æ–‡ä»¶ã€‚"""
        async with self._file_lock:
            try:
                # ä»…ä¿ç•™æœ€è¿‘ 10 æ¡ç¼“å­˜
                if len(cache) > 10:
                    sorted_keys = sorted(cache.keys())
                    cache = {k: cache[k] for k in sorted_keys[-10:]}
                self._atomic_write(self._cache_file, cache)
            except Exception as e:
                logger.error(f"ä¿å­˜æ€»ç»“ç¼“å­˜å¤±è´¥: {e}")

    async def terminate(self):
        """æ’ä»¶å¸è½½æ—¶å–æ¶ˆå®šæ—¶ä»»åŠ¡ã€‚"""
        if self._task and not self._task.done():
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        logger.info("æ¯æ—¥AIèµ„è®¯æ¨é€æ’ä»¶å·²åœç”¨")
