import asyncio
import json
import os
import time
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import List, Dict, Set

import aiohttp

from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.event import MessageChain
from astrbot.api.star import Context, Star, register
from astrbot.api import logger

# é»˜è®¤ RSS æºåˆ—è¡¨
DEFAULT_RSS_SOURCES = [
    "https://sspai.com/feed",                                               # å°‘æ•°æ´¾
    "https://www.huxiu.com/rss/0.xml",                                      # è™å—…
    "https://www.theverge.com/rss/ai-artificial-intelligence/index.xml",     # The Verge AI
    "https://feeds.feedburner.com/venturebeat/SZYF",                        # VentureBeat
    "https://www.marktechpost.com/feed/",                                   # MarkTechPost AI
]


@register(
    "astrbot_plugin_daily_ai_news",
    "YourName",
    "æ¯æ—¥AIèµ„è®¯è‡ªåŠ¨æ¨é€æ’ä»¶ï¼Œå®šæ—¶æŠ“å–å¤šä¸ª AI èµ„è®¯ RSS æºå¹¶æ¨é€åˆ° QQ ç¾¤",
    "1.0.0",
    "https://github.com/YourName/astrbot_plugin_daily_ai_news",
)
class DailyAINewsPlugin(Star):
    def __init__(self, context: Context):
        super().__init__(context)
        self._task: asyncio.Task = None
        self._subscriptions_file = os.path.join(
            "data", "astrbot_plugin_daily_ai_news", "subscriptions.json"
        )
        self._sent_file = os.path.join(
            "data", "astrbot_plugin_daily_ai_news", "sent_news.json"
        )
        # é€šè¿‡æŒ‡ä»¤è®¢é˜…çš„ unified_msg_origin é›†åˆ
        self._cmd_subscriptions: Set[str] = set()
        # å·²ç»æ¨é€è¿‡çš„æ–°é—»é“¾æ¥ï¼ˆç”¨äºå»é‡ï¼Œé¿å…é‡å¤æ¨é€ï¼‰
        self._sent_urls: Set[str] = set()

    async def initialize(self):
        """æ’ä»¶åˆå§‹åŒ–ï¼šåŠ è½½æŒä¹…åŒ–æ•°æ®ï¼Œå¯åŠ¨å®šæ—¶æ¨é€ä»»åŠ¡ã€‚"""
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(self._subscriptions_file), exist_ok=True)
        # åŠ è½½æŒä¹…åŒ–çš„è®¢é˜…åˆ—è¡¨
        self._load_subscriptions()
        # åŠ è½½å·²æ¨é€è®°å½•
        self._load_sent_news()
        # å¯åŠ¨åå°å®šæ—¶æ¨é€ä»»åŠ¡
        self._task = asyncio.create_task(self._schedule_loop())
        logger.info("æ¯æ—¥AIèµ„è®¯æ¨é€æ’ä»¶å·²åˆå§‹åŒ–")

    # ==================== æŒ‡ä»¤å¤„ç† ====================

    @filter.command("ainews")
    async def cmd_ainews(self, event: AstrMessageEvent):
        """æ‰‹åŠ¨è·å–æœ€æ–° AI èµ„è®¯"""
        yield event.plain_result("ğŸ”„ æ­£åœ¨è·å–æœ€æ–° AI èµ„è®¯ï¼Œè¯·ç¨å€™...")
        news_list = await self._fetch_all_news()
        if not news_list:
            yield event.plain_result("ğŸ˜ æš‚æ—¶æœªèƒ½è·å–åˆ° AI èµ„è®¯ï¼Œè¯·ç¨åå†è¯•ã€‚")
            return
        config = self.context.get_config()
        count = config.get("news_count", 10)
        text = self._format_news(news_list[:count])
        yield event.plain_result(text)

    @filter.command("ainews_sub")
    async def cmd_subscribe(self, event: AstrMessageEvent):
        """è®¢é˜…æ¯æ—¥ AI èµ„è®¯æ¨é€ï¼ˆåœ¨ç¾¤èŠä¸­ä½¿ç”¨ï¼‰"""
        umo = event.unified_msg_origin
        if umo in self._cmd_subscriptions:
            yield event.plain_result("ğŸ“¢ å½“å‰ä¼šè¯å·²è®¢é˜…æ¯æ—¥AIèµ„è®¯æ¨é€ã€‚")
            return
        self._cmd_subscriptions.add(umo)
        self._save_subscriptions()
        yield event.plain_result(
            "âœ… è®¢é˜…æˆåŠŸï¼æ¯æ—¥å°†è‡ªåŠ¨æ¨é€æœ€æ–°çš„ AI èµ„è®¯åˆ°æœ¬ç¾¤ã€‚\n"
            "å–æ¶ˆè®¢é˜…è¯·å‘é€ /ainews_unsub"
        )

    @filter.command("ainews_unsub")
    async def cmd_unsubscribe(self, event: AstrMessageEvent):
        """å–æ¶ˆæ¯æ—¥ AI èµ„è®¯æ¨é€è®¢é˜…"""
        umo = event.unified_msg_origin
        if umo not in self._cmd_subscriptions:
            yield event.plain_result("â„¹ï¸ å½“å‰ä¼šè¯æœªé€šè¿‡æŒ‡ä»¤è®¢é˜…è¿‡ AI èµ„è®¯æ¨é€ã€‚")
            return
        self._cmd_subscriptions.discard(umo)
        self._save_subscriptions()
        yield event.plain_result("âœ… å·²å–æ¶ˆæ¯æ—¥AIèµ„è®¯æ¨é€è®¢é˜…ã€‚")

    @filter.command("ainews_status")
    async def cmd_status(self, event: AstrMessageEvent):
        """æŸ¥çœ‹æ¨é€çŠ¶æ€"""
        config = self.context.get_config()
        hour = config.get("push_hour", 8)
        minute = config.get("push_minute", 0)
        count = config.get("news_count", 10)
        cmd_sub_count = len(self._cmd_subscriptions)
        cfg_groups = self._get_config_groups()
        cfg_group_count = len(cfg_groups)
        cfg_users = self._get_config_users()
        cfg_user_count = len(cfg_users)

        status_text = (
            "ğŸ“Š **æ¯æ—¥AIèµ„è®¯æ¨é€çŠ¶æ€**\n"
            f"â° æ¨é€æ—¶é—´ï¼šæ¯å¤© {hour:02d}:{minute:02d}\n"
            f"ğŸ“° æ¯æ¬¡æ¨é€ï¼š{count} æ¡\n"
            f"ğŸ“‹ æŒ‡ä»¤è®¢é˜…æ•°ï¼š{cmd_sub_count}\n"
            f"ğŸ“‹ é…ç½®ç¾¤å·æ•°ï¼š{cfg_group_count}\n"
            f"ğŸ“‹ é…ç½®ç§èŠæ•°ï¼š{cfg_user_count}\n"
            f"ğŸ“š å·²æ¨é€æ–°é—»ç¼“å­˜ï¼š{len(self._sent_urls)} æ¡"
        )
        yield event.plain_result(status_text)

    # ==================== å®šæ—¶æ¨é€ ====================

    async def _schedule_loop(self):
        """åå°å®šæ—¶å¾ªç¯ï¼Œæ¯å¤©åœ¨è®¾å®šæ—¶é—´æ¨é€æ–°é—»ã€‚"""
        while True:
            try:
                config = self.context.get_config()
                target_hour = config.get("push_hour", 8)
                target_minute = config.get("push_minute", 0)

                # è®¡ç®—è·ç¦»ä¸‹ä¸€æ¬¡æ¨é€çš„ç§’æ•°
                now = datetime.now()
                target = now.replace(
                    hour=target_hour, minute=target_minute, second=0, microsecond=0
                )
                if target <= now:
                    # ä»Šå¤©çš„æ¨é€æ—¶é—´å·²è¿‡ï¼Œæ¨åˆ°æ˜å¤©
                    target += timedelta(days=1)

                wait_seconds = (target - now).total_seconds()
                logger.info(
                    f"ä¸‹æ¬¡æ¨é€æ—¶é—´ï¼š{target.strftime('%Y-%m-%d %H:%M')}ï¼Œ"
                    f"ç­‰å¾… {wait_seconds:.0f} ç§’"
                )

                await asyncio.sleep(wait_seconds)

                # æ‰§è¡Œæ¨é€
                await self._do_push()

            except asyncio.CancelledError:
                logger.info("å®šæ—¶æ¨é€ä»»åŠ¡å·²å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(f"å®šæ—¶æ¨é€ä»»åŠ¡å‡ºé”™: {e}")
                # å‡ºé”™åç­‰å¾… 60 ç§’å†é‡è¯•
                await asyncio.sleep(60)

    async def _do_push(self):
        """æ‰§è¡Œä¸€æ¬¡æ–°é—»æ¨é€åˆ°æ‰€æœ‰è®¢é˜…ç›®æ ‡ã€‚"""
        logger.info("å¼€å§‹æ‰§è¡Œæ¯æ—¥AIèµ„è®¯æ¨é€...")

        news_list = await self._fetch_all_news()
        if not news_list:
            logger.warning("æœªèƒ½è·å–åˆ°ä»»ä½•æ–°é—»ï¼Œè·³è¿‡æœ¬æ¬¡æ¨é€")
            return

        config = self.context.get_config()
        count = config.get("news_count", 10)

        # è¿‡æ»¤æ‰å·²ç»æ¨é€è¿‡çš„æ–°é—»
        new_news = [n for n in news_list if n["link"] not in self._sent_urls]
        if not new_news:
            logger.info("æ²¡æœ‰æ–°çš„æœªæ¨é€æ–°é—»ï¼Œè·³è¿‡æœ¬æ¬¡æ¨é€")
            return

        selected = new_news[:count]
        text = self._format_news(selected)

        # è®°å½•å·²æ¨é€
        for n in selected:
            self._sent_urls.add(n["link"])
        # åªä¿ç•™æœ€è¿‘ 500 æ¡è®°å½•ï¼Œé¿å…æ— é™å¢é•¿
        if len(self._sent_urls) > 500:
            self._sent_urls = set(list(self._sent_urls)[-300:])
        self._save_sent_news()

        # åˆå¹¶æ‰€æœ‰éœ€è¦æ¨é€çš„ç›®æ ‡
        targets = self._get_all_targets()
        if not targets:
            logger.info("æ²¡æœ‰ä»»ä½•æ¨é€ç›®æ ‡ï¼Œè·³è¿‡æ¨é€")
            return

        # å‘é€åˆ°æ‰€æœ‰ç›®æ ‡
        for umo in targets:
            try:
                chain = MessageChain().message(text)
                await self.context.send_message(umo, chain)
                logger.info(f"å·²æ¨é€è‡³: {umo}")
            except Exception as e:
                logger.error(f"æ¨é€åˆ° {umo} å¤±è´¥: {e}")

        logger.info(f"æ¯æ—¥AIèµ„è®¯æ¨é€å®Œæˆï¼Œå…±æ¨é€ {len(selected)} æ¡æ–°é—»åˆ° {len(targets)} ä¸ªç›®æ ‡")

    # ==================== RSS æŠ“å– ====================

    async def _fetch_all_news(self) -> List[Dict]:
        """ä»æ‰€æœ‰é…ç½®çš„ RSS æºæŠ“å–æ–°é—»å¹¶åˆå¹¶æ’åºã€‚"""
        sources = self._get_rss_sources()
        all_news = []

        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30)
        ) as session:
            tasks = [self._fetch_rss(session, url) for url in sources]
            results = await asyncio.gather(*tasks, return_exceptions=True)

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.warning(f"RSS æº {sources[i]} æŠ“å–å¤±è´¥: {result}")
            elif result:
                all_news.extend(result)

        # å»é‡ï¼ˆæŒ‰é“¾æ¥ï¼‰
        seen = set()
        unique_news = []
        for item in all_news:
            if item["link"] not in seen:
                seen.add(item["link"])
                unique_news.append(item)

        # æŒ‰å‘å¸ƒæ—¶é—´é™åºæ’åˆ—
        unique_news.sort(key=lambda x: x.get("pub_time", ""), reverse=True)
        return unique_news

    async def _fetch_rss(self, session: aiohttp.ClientSession, url: str) -> List[Dict]:
        """æŠ“å–å•ä¸ª RSS æºå¹¶è§£æã€‚"""
        news_list = []
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (compatible; AstrBot-AI-News/1.0)"
            }
            async with session.get(url, headers=headers) as resp:
                if resp.status != 200:
                    logger.warning(f"RSS {url} è¿”å›çŠ¶æ€ç  {resp.status}")
                    return []
                text = await resp.text()

            root = ET.fromstring(text)

            # æ”¯æŒ RSS 2.0 å’Œ Atom æ ¼å¼
            # RSS 2.0
            items = root.findall(".//item")
            if items:
                for item in items:
                    title = self._get_xml_text(item, "title")
                    link = self._get_xml_text(item, "link")
                    pub_date = self._get_xml_text(item, "pubDate")
                    description = self._get_xml_text(item, "description")
                    if title and link:
                        news_list.append({
                            "title": title.strip(),
                            "link": link.strip(),
                            "pub_time": pub_date or "",
                            "summary": self._clean_html(description or ""),
                        })
                return news_list

            # Atom æ ¼å¼
            ns = {"atom": "http://www.w3.org/2005/Atom"}
            entries = root.findall(".//atom:entry", ns)
            if not entries:
                # å°è¯•æ— å‘½åç©ºé—´
                entries = root.findall(".//entry")
            for entry in entries:
                title = self._get_xml_text(entry, "title", ns)
                link_el = entry.find("atom:link", ns)
                if link_el is None:
                    link_el = entry.find("link")
                link = link_el.get("href", "") if link_el is not None else ""
                pub_date = (
                    self._get_xml_text(entry, "updated", ns)
                    or self._get_xml_text(entry, "published", ns)
                    or ""
                )
                summary = self._get_xml_text(entry, "summary", ns) or ""
                if title and link:
                    news_list.append({
                        "title": title.strip(),
                        "link": link.strip(),
                        "pub_time": pub_date,
                        "summary": self._clean_html(summary),
                    })

        except ET.ParseError as e:
            logger.warning(f"RSS XML è§£æå¤±è´¥ ({url}): {e}")
        except Exception as e:
            logger.warning(f"RSS æŠ“å–å¼‚å¸¸ ({url}): {e}")

        return news_list

    # ==================== å·¥å…·æ–¹æ³• ====================

    def _get_xml_text(self, element, tag, ns=None):
        """å®‰å…¨è·å– XML å­å…ƒç´ æ–‡æœ¬ã€‚"""
        if ns:
            for prefix, uri in ns.items():
                el = element.find(f"{{{uri}}}{tag}")
                if el is not None and el.text:
                    return el.text
        el = element.find(tag)
        return el.text if el is not None and el.text else None

    def _clean_html(self, text: str) -> str:
        """ç®€å•å»é™¤ HTML æ ‡ç­¾ã€‚"""
        import re
        clean = re.sub(r"<[^>]+>", "", text)
        clean = clean.replace("&nbsp;", " ").replace("&amp;", "&")
        clean = clean.replace("&lt;", "<").replace("&gt;", ">")
        clean = clean.replace("&quot;", '"')
        # æˆªå–å‰ 100 ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦
        clean = clean.strip()
        if len(clean) > 100:
            clean = clean[:100] + "..."
        return clean

    def _format_news(self, news_list: List[Dict]) -> str:
        """å°†æ–°é—»åˆ—è¡¨æ ¼å¼åŒ–ä¸ºæ¨é€æ–‡æœ¬ã€‚"""
        today = datetime.now().strftime("%Y-%m-%d")
        lines = [f"ğŸ“° æ¯æ—¥AIèµ„è®¯ | {today}\n{'=' * 28}\n"]

        for i, news in enumerate(news_list, 1):
            title = news["title"]
            link = news["link"]
            summary = news.get("summary", "")
            line = f"{i}. {title}\n   ğŸ”— {link}"
            if summary:
                line += f"\n   ğŸ“ {summary}"
            lines.append(line)

        lines.append(f"\n{'=' * 28}")
        lines.append("ğŸ’¡ å‘é€ /ainews éšæ—¶è·å–æœ€æ–°èµ„è®¯")
        return "\n\n".join(lines)

    def _get_rss_sources(self) -> List[str]:
        """è·å– RSS æºåˆ—è¡¨ï¼Œä¼˜å…ˆä½¿ç”¨é…ç½®ã€‚"""
        config = self.context.get_config()
        sources_text = config.get("rss_sources", "")
        if sources_text and sources_text.strip():
            sources = [
                s.strip() for s in sources_text.strip().split("\n") if s.strip()
            ]
            if sources:
                return sources
        return DEFAULT_RSS_SOURCES

    def _get_config_groups(self) -> List[str]:
        """ä»é…ç½®ä¸­è·å–æ‰‹åŠ¨å¡«å†™çš„ QQ ç¾¤å·åˆ—è¡¨ã€‚"""
        config = self.context.get_config()
        groups_text = config.get("subscribed_groups", "")
        if not groups_text or not groups_text.strip():
            return []
        return [g.strip() for g in groups_text.strip().split("\n") if g.strip()]

    def _get_config_users(self) -> List[str]:
        """ä»é…ç½®ä¸­è·å–æ‰‹åŠ¨å¡«å†™çš„ç§èŠ QQ å·åˆ—è¡¨ã€‚"""
        config = self.context.get_config()
        users_text = config.get("subscribed_users", "")
        if not users_text or not users_text.strip():
            return []
        return [u.strip() for u in users_text.strip().split("\n") if u.strip()]

    def _get_all_targets(self) -> Set[str]:
        """
        è·å–æ‰€æœ‰æ¨é€ç›®æ ‡çš„ unified_msg_originã€‚
        åˆå¹¶æŒ‡ä»¤è®¢é˜…ã€é…ç½®ç¾¤å·ã€é…ç½®ç§èŠä¸‰ç§æ¥æºã€‚
        """
        targets = set(self._cmd_subscriptions)

        # å°†é…ç½®ä¸­çš„ç¾¤å·è½¬æ¢ä¸º unified_msg_origin æ ¼å¼
        cfg_groups = self._get_config_groups()
        for group_id in cfg_groups:
            umo = f"aiocqhttp:GroupMessage:{group_id}"
            targets.add(umo)

        # å°†é…ç½®ä¸­çš„ç§èŠ QQ å·è½¬æ¢ä¸º unified_msg_origin æ ¼å¼
        cfg_users = self._get_config_users()
        for user_id in cfg_users:
            umo = f"aiocqhttp:FriendMessage:{user_id}"
            targets.add(umo)

        return targets

    # ==================== æŒä¹…åŒ– ====================

    def _load_subscriptions(self):
        """ä»æ–‡ä»¶åŠ è½½æŒ‡ä»¤è®¢é˜…åˆ—è¡¨ã€‚"""
        try:
            if os.path.exists(self._subscriptions_file):
                with open(self._subscriptions_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                self._cmd_subscriptions = set(data.get("subscriptions", []))
                logger.info(f"å·²åŠ è½½ {len(self._cmd_subscriptions)} ä¸ªæŒ‡ä»¤è®¢é˜…")
        except Exception as e:
            logger.error(f"åŠ è½½è®¢é˜…åˆ—è¡¨å¤±è´¥: {e}")
            self._cmd_subscriptions = set()

    def _save_subscriptions(self):
        """å°†æŒ‡ä»¤è®¢é˜…åˆ—è¡¨ä¿å­˜åˆ°æ–‡ä»¶ã€‚"""
        try:
            with open(self._subscriptions_file, "w", encoding="utf-8") as f:
                json.dump(
                    {"subscriptions": list(self._cmd_subscriptions)},
                    f,
                    ensure_ascii=False,
                    indent=2,
                )
        except Exception as e:
            logger.error(f"ä¿å­˜è®¢é˜…åˆ—è¡¨å¤±è´¥: {e}")

    def _load_sent_news(self):
        """åŠ è½½å·²æ¨é€æ–°é—»è®°å½•ã€‚"""
        try:
            if os.path.exists(self._sent_file):
                with open(self._sent_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                self._sent_urls = set(data.get("sent_urls", []))
                logger.info(f"å·²åŠ è½½ {len(self._sent_urls)} æ¡å·²æ¨é€è®°å½•")
        except Exception as e:
            logger.error(f"åŠ è½½å·²æ¨é€è®°å½•å¤±è´¥: {e}")
            self._sent_urls = set()

    def _save_sent_news(self):
        """ä¿å­˜å·²æ¨é€æ–°é—»è®°å½•ã€‚"""
        try:
            with open(self._sent_file, "w", encoding="utf-8") as f:
                json.dump(
                    {"sent_urls": list(self._sent_urls)},
                    f,
                    ensure_ascii=False,
                    indent=2,
                )
        except Exception as e:
            logger.error(f"ä¿å­˜å·²æ¨é€è®°å½•å¤±è´¥: {e}")

    async def terminate(self):
        """æ’ä»¶å¸è½½æ—¶å–æ¶ˆå®šæ—¶ä»»åŠ¡ã€‚"""
        if self._task and not self._task.done():
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        logger.info("æ¯æ—¥AIèµ„è®¯æ¨é€æ’ä»¶å·²åœç”¨")
